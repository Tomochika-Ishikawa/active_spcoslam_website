<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Active SpCoSLAM">
  <meta property="og:title" content="Active SpCoSLAM"/>
  <meta property="og:description" content="Active SpCoSLAM"/>
  <meta property="og:url" content="https://tomochika-ishikawa.github.io/active_spcoslam_website/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/graphical_model.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Active SpCoSLAM">
  <meta name="twitter:description" content="Active SpCoSLAM">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/graphical_model.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Active SpCoSLAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Active SpCoSLAM</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Active Semantic Mapping for Household Robots: Rapid Indoor Adaptation and Reduced User Burden</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              Tomochika Ishikawa<sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=jtB7J0AAAAAJ&hl=ja&oi=ao" target="_blank">Akira Taniguchi<sup>1</sup>,</a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=Y4qjYvMAAAAJ&hl=ja&oi=ao" target="_blank">Yoshinobu Hagiwara<sup>1,2</sup>,</a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=dPOCLQEAAAAJ&hl=ja&oi=ao" target="_blank">Tadahiro Taniguchi<sup>1,3</sup></a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Ritsumeikan University,
              <sup>2</sup>Soka University,
              <sup>3</sup>Kyoto University
              <br><span>Accepted at IEEE SMC 2023.</span>
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://ieeexplore.ieee.org/abstract/document/10394143?casa_token=MqmOILFfJBwAAAAA:eB8xOuq_LBVvT7c4LD-5Nvn4gioHG_-ULz5ax9UU8f4XE-DA6HG5RcysAIMNkGpsVMgkEOE" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>
                    Paper
                  </span>
                </a>
              </span>

              <!-- Slide link -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1BGDmYK7fahyv-2aKla5YY8fmZFaZ5izd/view" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slide</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/Tomochika-Ishikawa/Active-SpCoSLAM" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Demonstration Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/vv9k_wPkfIs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->
  
<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Active semantic mapping is essential for service robots to quickly capture both the map of an environment and its spatial meaning, while also minimizing the burden on users during robot operation and data collection. SpCoSLAM, a method of semantic mapping with place categorization and simultaneous localization and mapping (SLAM), is well suited to environmental adaptation, as it is not limited to predefined labels. However, SpCoSLAM presents two issues that increase the burden on users: 1) users struggle to efficiently determine a destination for the robot’s quick adaptation, and 2) providing instructions to the robot becomes repetitive and cumbersome. To address these challenges, we propose Active-SpCoSLAM, which enables the robot to actively explore uncharted areas and employs CLIP for image captioning to provide a flexible vocabulary that replaces human instructions. The robot determines its actions by calculating information gain integrated from both semantics and SLAM uncertainties. We conducted experiments in a simulated environment, comparing the proposed method to other methods in terms of efficiency and applicability to object discovery tasks. Additionally, we tested the proposed method, which combines user instruction and CLIP, in a real environment. Our results demonstrated that the robot explored its environment with approximately five fewer iterations and 11 minutes faster compared to the case of random exploration. Moreover, our method achieved a higher success rate in object discovery tasks during earlier stages of learning compared to other methods. In conclusion, the proposed method rapidly covers an environment while gathering useful data for object discovery tasks, thus reducing the burden on users and enhancing the robot’s adaptability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Graphical Model</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item" style="text-align: center;">
          <img src="./static/images/graphical_model.png" alt="image of model" style="width: 70%; height: auto; display: block; margin: 0 auto;"/>
          <h2 class="subtitle" style="text-align: left; margin-top: 20px;">
            Focused task in this study. 
            (Top) The robot performs SLAM to estimate self-position and map simultaneously.
            (Bottom) The robot acquires categorical knowledge (spatial concepts) about locations based on multimodal information observed by the robot: image features, image captions, and self-location.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Action Determination</h2>
      <!--       <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/information_gain_demo.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            The robot navigates to the position with the maximum information gain among the candidate points.
            The IG in Active-SpCoSLAM consists of a weighted sum of three IGs: related to spatial concepts, self-location, and maps, respectively.
          </h2>
          
        </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Video carouse2 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Learning Spatial Concept</h2>
      <!--       <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/spatial_concept.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            First, the robot acquires multiple images of its surroundings with the head camera.
            Then, the images are input to ClipCap for caption generation, and the images are input to CNN to obtain image features.
            The robot learns location concepts by categorizing these, plus self-position, into three multimodal pieces of information.
          </h2>
          
        </div>
    </div>
  </div>
</section>
<!-- End video carouse2 -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/n8se-MgPi50" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>BibTex Code Here</code></pre> -->
    <pre><code>
      @inproceedings{ishikawa2023active,
        title={Active semantic mapping for household robots: rapid indoor adaptation and reduced user burden},
        author={Ishikawa, Tomochika and Taniguchi, Akira and Hagiwara, Yoshinobu and Taniguchi, Tadahiro},
        booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
        pages={3116--3123},
        year={2023},
        organization={IEEE}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<!--Acknowledgements citation -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <!-- <h2 class="title">Acknowledgements</h2> -->
    <h2 class="title">Funding</h2>
    <p>
      This work was supported by the Japan Science and Technology Agency (JST), Moonshot Research & Development Program (Grant Number JPMJMS2011), and the Japan Society for the Promotion of Science (JSPS), KAKENHI Grant Number JP20K19900, JP23K16975 and JP22K12212.
    </p>
  </div>
</section>
<!--End Acknowledgements citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
